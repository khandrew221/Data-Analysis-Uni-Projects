{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've seen the basics of how Mongo works, let's import and process a larger dataset.\n",
    "\n",
    "In this notebook, you will look at some of the issues in importing data from CSV files into MongoDB. We'll use the data from the [Ultimate Doctor Who](http://www.ultimatedoctorwho.com/) site, though with some modifications to remove duplicate column names in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the document database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebooks for parts 14, 15 and 16, you will be using a document database to manage data. As with the relational database you looked at in previous sections, the data in the database is *persistent*. The document database, MongoDB, is described as \"NoSQL\" to reflect that it does not use the tabular format of the relational database to store data. However, many of properties of a formal RDBMS apply to MongoDB, including the need to connect to the database server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with PostgreSQL, the MongoDB database server runs independently from the Jupyter notebook server. To interact with it, you need to set up an explicit connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting your database credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with a database, we need to create a *connection* to the database. A connection allows us to manipulate the database, and query its contents (depending on what usage rights you have been granted). For the SQL notebooks in TM351, the details of your connection will depend upon whether you are using the OU-hosted server, accessed via [tm351.open.ac.uk](https:tm351.open.ac.uk), or whether you are using a version hosted on your own computer, which you should have set up using either Vagrant or Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the connection, you need a login name and a pasword. we will use the variables `DB_USER` and `DB_PWD` to hold the user name and password respectively that you will use to connect to the database. Run the appropriate cell to set your credentials in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to the database on [tm351.open.ac.uk](https:tm351.open.ac.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the Open University hosted server, you should execute the following cell, using your OUCU as the value of `DB_USER`, and the password you were given at the beginning of the module. Note that if the cell is in RAW NBconvert style, you will need to change its type to Code in order to execute it.\n",
    "\n",
    "The variables `DB_USER` and `DB_PWD` are strings, and so you need to put them in quotes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If you are using the remote environment, change this cell\n",
    "# type to \"code\", and execute it\n",
    "\n",
    "DB_USER='xxx99'            # Enter your OUCU here (in quotes)\n",
    "DB_PWD='your_password'     # Enter your password here (in quotes)\n",
    "\n",
    "import urllib\n",
    "\n",
    "MONGO_CONNECTION_STRING = f\"mongodb://{DB_USER}:{urllib.parse.quote_plus(DB_PWD)}@localhost:27017/?authsource=user-data\"\n",
    "print(f\"MONGO_CONNECTION_STRING = {MONGO_CONNECTION_STRING}\")\n",
    "\n",
    "DB_NAME=DB_USER\n",
    "print(f\"DB_NAME = {DB_NAME}\")\n",
    "\n",
    "ACCIDENTS_DB_NAME=\"accidents-2012\"\n",
    "# ACCIDENTS_DB_NAME=\"accidents-09-12\" # Uncomment this line to use the full accident database\n",
    "\n",
    "print(f\"ACCIDENTS_DB_NAME = {ACCIDENTS_DB_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, note that the connection string contains an additional option at the end: `?authsource=user-data`. For the MongoDB setup that we are using here, this option tells Mongo where to look for the authentication database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to the database on a locally hosted machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running the Jupyter server on your own machine, via Docker or Vagrant, you should execute the following cell. Note that if the cell is in RAW NBconvert style, you will need to change its type to Code in order to execute it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If you are using a locally hosted environment, change this cell\n",
    "# type to \"code\", and execute it\n",
    "\n",
    "MONGO_CONNECTION_STRING = f\"mongodb://localhost:27017/\"\n",
    "print(f\"MONGO_CONNECTION_STRING = {MONGO_CONNECTION_STRING}\")\n",
    "\n",
    "DB_NAME=\"test_db\"  # For a local VCE, this can be any value\n",
    "print(f\"DB_NAME = {DB_NAME}\")\n",
    "\n",
    "ACCIDENTS_DB_NAME=\"accidents\"\n",
    "print(f\"ACCIDENTS_DB_NAME = {ACCIDENTS_DB_NAME}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the locally hosted versions of the environment give you full administrator rights, which is why you do not need to specify a user name or password. Obviously, this would not generally not be granted on a multi-user database, unless you are the database administrator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set up a connection to the database. As with PostgreSQL, we use a connection string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MONGO_CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connection string is made up of several parts:\n",
    "\n",
    "- `mongodb` : tells `pymongo` that we will use MongoDB as our database engine\n",
    "- Your user name and (character escaped) password, separated by a colon if you are using the remote server. If you are using a local server, you will be logged on as an adminstrator, and do not need to specify a name or password.\n",
    "- `localhost:27017` : the port on which the database engine is listening.\n",
    "- A reference to the authentication file (`?authsource=user-data`), if you are using the remote server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now connect to the database with a `pymongo.MongoClient` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client=MongoClient(MONGO_CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be connected to the MongoDB database server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be connected to the MongoDB database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data from a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at the data in the CSV file. The csv file is in the file `Ultimate_Doctor_Who_resave.csv` in the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_who_df=pd.read_csv('data/Ultimate_Doctor_Who_resave.csv')\n",
    "\n",
    "dr_who_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will start by creating the database client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_db=mongo_client[DB_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create a collection called `dr_who_collection`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_db.drop_collection('dr_who_collection')\n",
    "dw_collection=mongo_db['dr_who_collection']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what happens when we try to insert the data in the dataframe into the MongoDB collection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_collection.insert_many(dr_who_df.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the first problem is that several of the column names contain full stops, which, as we saw in notebook `14.2 Working With Embedded Documents` is not permitted in MongoDB.\n",
    "\n",
    "One solution to this might simply be to replace any full stops in the column headings with a safer character such as an underscore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_who_safe_df=dr_who_df.rename(lambda s:s.replace('.', '_'), axis='columns')\n",
    "\n",
    "dr_who_safe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many circumstances, this may be the most sensible approach, but in this case there are some better techniques we can use.\n",
    "\n",
    "Looking at the column headings which contain a full stop, we can see that these are all parts of sequences (eg. the different parts of a given series).\n",
    "\n",
    "So rather than deal with the full stops at this point, let's begin by inserting only that information which isn't part of a numbered list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_who_df[['Story ID',\n",
    " 'Year',\n",
    " 'Season',\n",
    " 'Title',\n",
    " 'Type of Broadcast',\n",
    " 'Doctor Number',\n",
    " 'Doctor',\n",
    " 'Guest Doctor(s)',\n",
    " 'Appearance of UNIT',\n",
    " 'Recurring Villains',\n",
    " 'Firsts']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in notebook `14.1 Basic CRUD`: we can insert this data into the MongoDB database using the `.to_dict(orient='records')` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_collection.insert_many(dr_who_df[['Story ID',\n",
    " 'Year',\n",
    " 'Season',\n",
    " 'Title',\n",
    " 'Type of Broadcast',\n",
    " 'Doctor Number',\n",
    " 'Doctor',\n",
    " 'Guest Doctor(s)',\n",
    " 'Appearance of UNIT',\n",
    " 'Recurring Villains',\n",
    " 'Firsts']].to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_collection.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice at this point that some fields contain a NULL value (shown here as `nan`). We should remove these values: in a MongoDB database, the NULL should be reflected simply by the lack of the relevant key, rather than an explicit NULL value. In this case, we can make a comparison with the python expression `float('nan')`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dw_collection.find({'Guest Doctor(s)': float('nan')}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the NULL values from the `Guest Doctor(s)` column, we use `$unset`, as described in notebook `14.1 Basic CRUD`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_collection.update_many({'Guest Doctor(s)': float('nan')}, {'$unset': {'Guest Doctor(s)':''}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now look at an early story, we find that the document no longer contains the  `Guest Doctor(s)` key. This is how NULL is represented in MongoDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_collection.find_one({'Story ID': 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see whether a document actually contains a given key, use the condition `{'$exists': True}`. So to find a document which does contain a key and value for  `Guest Doctor(s)`, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_collection.find_one({'Guest Doctor(s)': {'$exists': True}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to find a document which does not contain the key, just use the condition `{'$exists': False}`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_collection.find_one({'Guest Doctor(s)': {'$exists': False}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### Activity 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "For the rest of the documents in `dw_collection`, remove any NULL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Write your code in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "heading_collapsed": true
   },
   "source": [
    "#### Our solution\n",
    "\n",
    "To reveal our solution, run this cell or click on the triangle symbol on the left-hand side of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "As all documents were created with the same set of keys, we can use the remaining keys in any arbitrary document to find the remaining keys to remove NULL values from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dw_collection.find_one({}).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "We can use this list to clear the NULL values from the rest of the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for key in dw_collection.find_one({}).keys():\n",
    "    dw_collection.update_many({key: float('nan')}, {'$unset': {key:''}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "If we now look at the first document again, we should see that the NULL entries have been removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dw_collection.find_one({'Story ID': 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "#### End of Activity 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays in MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the fields in the database would be better represented as an ordered collection than by a key-value paired json subdocument. For this data, we can use a list, or an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the column names in the original csv file, we see that there are a number of columns for the doctor's companions in any given story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_who_df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns `Companion 1`, `Companion 2` and so on allow for up to 8 companions to be included for each story, but require several named columns in a csv file, many of which will be completely populated with NULL values. A more natural way of representing the companions in each episode would be with an array.\n",
    "\n",
    "To include the companions in each story in an array, we can write a function which takes a row of a DataFrame (which is a series), and use `'$set'` to add it to the relevant document in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_companions(row_ss):\n",
    "    '''Takes a row from the dr_who_df dataframe\n",
    "    and return a list of all companions'''\n",
    "    out=[]\n",
    "    for c in row_ss.index:\n",
    "        if c[:9]=='Companion' and pd.notnull(row_ss[c]):\n",
    "            out.append(row_ss[c])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the first row of the dataframe is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_who_df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the list of companions is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_companions(dr_who_df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add the companion data to the database collection, apply the function to all rows in the DataFrame, and use `'$set'` to add to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in dr_who_df.index:\n",
    "    row=dr_who_df.loc[idx]\n",
    "    dw_collection.update_one({'Story ID':row['Story ID']}, \n",
    "                             {'$set': {'Companions':find_companions(row)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_collection.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a similar process with the information about each of the parts. Rather than have keys named `'Pt. 1 air date'`, `'Pt.3 viewers'` and the like, we can create an array of subdocuments for each story. Then each subdocument can contain the air date and viewer numbers for the particular episode. Note that this also means that all the column names containing full stops will have been handled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (rather verbose) function for extracting the relevant episode information might be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_info(row_ss):\n",
    "    '''Takes a row from the dr_who_df dataframe\n",
    "    and return a list of the part information'''\n",
    "    parts_ls=[]\n",
    "    \n",
    "    if not(pd.isnull(row_ss['Pt. 1 air date'])):\n",
    "        parts_ls.append({'number':1,\n",
    "                         'air_date':row_ss['Pt. 1 air date'],\n",
    "                         'viewers':row_ss['Pt. 1 viewers (in millons)']})\n",
    "        \n",
    "    if not(pd.isnull(row_ss['Pt. 2 air date'])):\n",
    "        parts_ls.append({'number':2, \n",
    "                         'air_date':row_ss['Pt. 2 air date'],\n",
    "                         'viewers':row_ss['Pt.2 viewers']})\n",
    "        \n",
    "    if not(pd.isnull(row_ss['Pt. 3 air date'])):\n",
    "        parts_ls.append({'number':3,\n",
    "                         'air_date':row_ss['Pt. 3 air date'],\n",
    "                         'viewers':row_ss['Pt.3 viewers']})\n",
    "    \n",
    "    if not(pd.isnull(row_ss['Pt. 4 air date'])):\n",
    "        parts_ls.append({'number':4,\n",
    "                         'air_date':row_ss['Pt. 4 air date'],\n",
    "                         'viewers':row_ss['Pt.4 viewers']})\n",
    "    \n",
    "    if not(pd.isnull(row_ss['Pt.5 air date'])):\n",
    "        parts_ls.append({'number':5,\n",
    "                         'air_date':row_ss['Pt.5 air date'],\n",
    "                         'viewers':row_ss['Pt. 5 viewers']})\n",
    "    \n",
    "    if not(pd.isnull(row_ss['Pt.6 air date'])):\n",
    "        parts_ls.append({'number':6,\n",
    "                         'air_date':row_ss['Pt.6 air date'],\n",
    "                         'viewers':row_ss['Pt.6 viewers']})\n",
    "    \n",
    "    if not(pd.isnull(row_ss['Pt. 7 air date'])):\n",
    "        parts_ls.append({'number':7,\n",
    "                         'air_date':row_ss['Pt. 7 air date'],\n",
    "                         'viewers':row_ss['Pt.7 viewers']})\n",
    "    \n",
    "    if not(pd.isnull(row_ss['pt. 8 air date'])):\n",
    "        parts_ls.append({'number':8, \n",
    "                         'air_date':row_ss['pt. 8 air date'],\n",
    "                         'viewers':row_ss['pt. 8 viewers']})\n",
    "    \n",
    "    if not(pd.isnull(row_ss['pt. 9 air date'])):\n",
    "        parts_ls.append({'number':9,\n",
    "                         'air_date':row_ss['pt. 9 air date'],\n",
    "                         'viewers':row_ss['pt. 9 viewers']})\n",
    "    \n",
    "    if not(pd.isnull(row_ss['pt. 10 air date'])):\n",
    "        parts_ls.append({'number':10,\n",
    "                         'air_date':row_ss['pt. 10 air date'],\n",
    "                         'viewers':row_ss['pt. 10 viewers']})\n",
    "    \n",
    "    if not(pd.isnull(row_ss['pt. 11 air date'])):\n",
    "        parts_ls.append({'number':11,\n",
    "                         'air_date':row_ss['pt. 11 air date'],\n",
    "                         'viewers':row_ss['pt. 11 viewers']})\n",
    "    \n",
    "    if not(pd.isnull(row_ss['pt. 12 air date'])):\n",
    "        parts_ls.append({'number':12,\n",
    "                         'air_date':row_ss['pt. 12 air date'],\n",
    "                         'viewers':row_ss['pt. 12 viewers']})\n",
    "    \n",
    "    return parts_ls\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** *If you are confident programming in python, you might be thinking that there are far more efficient ways to work through the cases, for example by matching and iterating across the column names. This is certainly true, and you should feel free to adapt the code according to your coding level and style.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call the `get_part_info` function on the first row of the DataFrame, we receive a list of four parts, with the air date and viewer numbers (in millions) for each of those parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_part_info(dr_who_df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as with the companion arrays, we can add these to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in dr_who_df.index:\n",
    "    row=dr_who_df.loc[idx]\n",
    "    dw_collection.update_one({'Story ID':row['Story ID']}, \n",
    "                             {'$set': {'Parts':get_part_info(row)}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the parts are represented by an array of subdocuments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_collection.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we want to find the details of the episode which was aired on 25th April, 1964, we use the dotted notation, as described in notebook `14.2 Working With Embedded Documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_collection.find_one({'Story ID':5, 'Parts.air_date':'4/25/64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have made a reasonable first attempt at converting the initial csv file into an appropriate structure for a MongoDB database. Where a value does not exist for a particular document, we have removed the key rather than have a NULL entry. Similarly, where the csv file had many columns to represent fields with multiple values, we have represented the information with an array.\n",
    "\n",
    "Something to be wary of when using MongoDB with pandas, is that the DataFrame will still create columns for missing data. If we fun a `.find({})` query on the collection now, and cast into a DataFrame, the resulting DataFrame has columns for `Firsts`, `Guest Doctor(s)` and so on, even though these keys do not appear in the majority of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dw_collection.find({})).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is important to remember that it may possible to misinterpret how the DataFrame represents the results of the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-success"
    ]
   },
   "source": [
    "Cleaning data is often an iterative process. If you have a known document schema against which you can check your data, that provides a good benchmark against which to check your data. But in many cases, you will have to use your own judgement on what data type any data field is best represented by.\n",
    "\n",
    "You are also likely to find that data cleaning is an iterative process. The more you look at your dataset, the more dirty you may realise that it is.\n",
    "\n",
    "For example, for this dataset, there are some spelling mistakes in the entries (for example, \"Patrick Traughton\" is listed as a guest doctor, rather than \"Patrick Troughton\"), we have not converted the (string) entries into more appropriate types, such as floats or datetimes, there are (despite our earlier efforts) still NULL entries in the set, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-warning"
    ]
   },
   "source": [
    "*If you are interested, you could try to further clean the data if there are other ways of storing it that you would like to explore.*\n",
    "\n",
    "*Remember, the view of the data presented by the dataframe may differ from the data that is actual stored in the database documents.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can drop this test collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": false
   },
   "outputs": [],
   "source": [
    "mongo_client[DB_NAME].drop_collection('dr_who_collection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if you are working on a local VCE, you can also drop the database you created (if you are working on the remote VCE, you do not have permission to drop your database):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will not work on the remote VCE\n",
    "mongo_client.drop_database(DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": false
   },
   "source": [
    "## What next?\n",
    "If you are working through this Notebook as part of an inline exercise, return to the module materials now.\n",
    "\n",
    "If you are working through this set of Notebooks as a whole, move on to `14.4 Introduction to the accidents database`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
