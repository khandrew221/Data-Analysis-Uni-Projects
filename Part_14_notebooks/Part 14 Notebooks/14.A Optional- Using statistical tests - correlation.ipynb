{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using statistical tests â€” correlation (optional)\n",
    "\n",
    "This Notebook will show some examples of using statistical tests to get qualitative results from questions asked of the accidents dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-success"
    ]
   },
   "source": [
    "*To be in any way meaningful, rather than misleading, statistical methods need to be treated with care and used in an appropriate way. __This module is not intended to teach you statistical methods.__*\n",
    "\n",
    "*The examples provided here should be regarded primarily as examples of how to apply powerful statistics based Python packages to datasets, rather than examples of how to \"do statistics\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we'll be using analysing data grabbed from a MongoDB database into *pandas* dataframes, with all the risks that implies from the point of view of polluting the results data with artefacts arising from the re-presentation of of the potentially irregular results documents in regular, tabular form. The statistical analyses will be performed using functions from the `scipy.stats` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": false
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the document database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebooks for parts 14, 15 and 16, you will be using a document database to manage data. As with the relational database you looked at in previous sections, the data in the database is *persistent*. The document database, MongoDB, is described as \"NoSQL\" to reflect that it does not use the tabular format of the relational database to store data. However, many of properties of a formal RDBMS apply to MongoDB, including the need to connect to the database server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with PostgreSQL, the MongoDB database server runs independently from the Jupyter notebook server. To interact with it, you need to set up an explicit connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting your database credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with a database, we need to create a *connection* to the database. A connection allows us to manipulate the database, and query its contents (depending on what usage rights you have been granted). For the SQL notebooks in TM351, the details of your connection will depend upon whether you are using the OU-hosted server, accessed via [tm351.open.ac.uk](https:tm351.open.ac.uk), or whether you are using a version hosted on your own computer, which you should have set up using either Vagrant or Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the connection, you need a login name and a pasword. we will use the variables `DB_USER` and `DB_PWD` to hold the user name and password respectively that you will use to connect to the database. Run the appropriate cell to set your credentials in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to the database on [tm351.open.ac.uk](https:tm351.open.ac.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the Open University hosted server, you should execute the following cell, using your OUCU as the value of `DB_USER`, and the password you were given at the beginning of the module. Note that if the cell is in RAW NBconvert style, you will need to change its type to Code in order to execute it.\n",
    "\n",
    "The variables `DB_USER` and `DB_PWD` are strings, and so you need to put them in quotes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If you are using the remote environment, change this cell\n",
    "# type to \"code\", and execute it\n",
    "\n",
    "DB_USER='xxx99'            # Enter your OUCU here (in quotes)\n",
    "DB_PWD='your_password'     # Enter your password here (in quotes)\n",
    "\n",
    "import urllib\n",
    "\n",
    "MONGO_CONNECTION_STRING = f\"mongodb://{DB_USER}:{urllib.parse.quote_plus(DB_PWD)}@localhost:27017/?authsource=user-data\"\n",
    "print(f\"MONGO_CONNECTION_STRING = {MONGO_CONNECTION_STRING}\")\n",
    "\n",
    "DB_NAME=DB_USER\n",
    "print(f\"DB_NAME = {DB_NAME}\")\n",
    "\n",
    "ACCIDENTS_DB_NAME=\"accidents-2012\"\n",
    "# ACCIDENTS_DB_NAME=\"accidents-09-12\" # Uncomment this line to use the full accident database\n",
    "\n",
    "print(f\"ACCIDENTS_DB_NAME = {ACCIDENTS_DB_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, note that the connection string contains an additional option at the end: `?authsource=user-data`. For the MongoDB setup that we are using here, this option tells Mongo where to look for the authentication database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to the database on a locally hosted machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running the Jupyter server on your own machine, via Docker or Vagrant, you should execute the following cell. Note that if the cell is in RAW NBconvert style, you will need to change its type to Code in order to execute it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If you are using a locally hosted environment, change this cell\n",
    "# type to \"code\", and execute it\n",
    "\n",
    "MONGO_CONNECTION_STRING = f\"mongodb://localhost:27017/\"\n",
    "print(f\"MONGO_CONNECTION_STRING = {MONGO_CONNECTION_STRING}\")\n",
    "\n",
    "DB_NAME=\"test_db\"  # For a local VCE, this can be any value\n",
    "print(f\"DB_NAME = {DB_NAME}\")\n",
    "\n",
    "ACCIDENTS_DB_NAME=\"accidents\"\n",
    "print(f\"ACCIDENTS_DB_NAME = {ACCIDENTS_DB_NAME}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the locally hosted versions of the environment give you full administrator rights, which is why you do not need to specify a user name or password. Obviously, this would not generally not be granted on a multi-user database, unless you are the database administrator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set up a connection to the database. As with PostgreSQL, we use a connection string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MONGO_CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connection string is made up of several parts:\n",
    "\n",
    "- `mongodb` : tells `pymongo` that we will use MongoDB as our database engine\n",
    "- Your user name and (character escaped) password, separated by a colon if you are using the remote server. If you are using a local server, you will be logged on as an adminstrator, and do not need to specify a name or password.\n",
    "- `localhost:27017` : the port on which the database engine is listening.\n",
    "- A reference to the authentication file (`?authsource=user-data`), if you are using the remote server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now connect to the database with a `pymongo.MongoClient` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client=MongoClient(MONGO_CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be connected to the MongoDB database server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The accidents database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accidents database takes a long time to set up, so we have already imported it into a MongoDB database so that you can work with it. Note that on the remote VCE, the database is read-only, so you will not be able to alter its contents, although you can copy the contents into your own database space as discussed in the previous MongoDB notebooks, and alter that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells in the earlier section, Setting up the document database, put the name of the accidents database into the variable `ACCIDENTS_DB_NAME`. Use this value to set up the connection to the `accidents` database and collections within it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_db=mongo_client[ACCIDENTS_DB_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the names of the collections in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_db.list_collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will introduce some of the different collections in the rest of the materials, but let's start with the `accidents` collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_collection=accidents_db['accidents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This collection contains information on individual accidents. We can see how many examples it contains with the `.count_documents()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_collection.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a larger plot size than the default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the number of casualties and vehicles\n",
    "in Notebook `14.2 Introduction to accidents`, we presented a simple visual analysis comparing the number of casualties with the number of vehicles across a large number of accidents.\n",
    "\n",
    "The following code retrieves the relevant data from the database and puts it into a long form with two relevant columns: the number of casualties and the number of vehicles. Each row corresponds to a separate accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": false
   },
   "outputs": [],
   "source": [
    "# Build a DataFrame, one row for each accident\n",
    "cas_veh_df = pd.DataFrame(accidents_collection.find({'Local_Authority_(Highway)': 'E06000042'},\n",
    "                                                 ['Number_of_Casualties', 'Number_of_Vehicles']))\n",
    "cas_veh_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noted before, the regression line is a simple best fit line, calculated over the raw, item level accident data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": false
   },
   "outputs": [],
   "source": [
    "regressionline = scipy.stats.linregress(cas_veh_df['Number_of_Casualties'],\n",
    "                                        cas_veh_df['Number_of_Vehicles'])\n",
    "\n",
    "# The regression line is of the form y = m x + b\n",
    "m = regressionline[0]\n",
    "b = regressionline[1]\n",
    "(m, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also force the data into a different long form with three columns: the number of casualties, the number of vehicles, and the count of accidents with that *(casualty, vehicle)* combination. The data in this format can then easily be plotted as a bubble chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count the number of each severity\n",
    "cas_veh_crosstab = pd.crosstab(cas_veh_df['Number_of_Casualties'],\n",
    "                               cas_veh_df['Number_of_Vehicles'])\n",
    "# Reshape\n",
    "cas_veh_crosstab = cas_veh_crosstab.stack().reset_index()\n",
    "cas_veh_crosstab.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chart demonstrates how the count data and the regression line can be plotted on the same chart using a matplotlib chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(cas_veh_crosstab['Number_of_Casualties'], \n",
    "            cas_veh_crosstab['Number_of_Vehicles'],\n",
    "            s = np.sqrt(cas_veh_crosstab[0])*50,\n",
    "            alpha=0.5\n",
    "            )\n",
    "\n",
    "x = np.linspace(0, 30, 20)\n",
    "plt.plot(x, m*x + b)\n",
    "\n",
    "plt.xlabel('Number of casualties')\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can create the chart directly from the original line item dataset and let the chart do the counting for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best fit/linear regression chart may take some time to calculate\n",
    "sns.lmplot(x=\"Number_of_Casualties\", y=\"Number_of_Vehicles\", data=cas_veh_df,\n",
    "           x_jitter=0.2, y_jitter=0.2, scatter_kws={'s':1}\n",
    "          )\n",
    "\n",
    "# Set the x axis limits (None means set automatically)\n",
    "plt.xlim(0, None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst he best fit line might suggest there is an increasing relationship between the number of casualties and the number of vehicles, the scatter / bubble marks on the chart suggest otherwise.\n",
    "\n",
    "So is there a better, mathematical, way of identifying whether there is a relationship or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Correlation Coefficients\n",
    "\n",
    "To measure the strength of the relationship between two sets of numbers, we can use a statistical measure know as a *correlation coefficient*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson's *r* - \"The\" Correlation Coefficient\n",
    "\n",
    "`Pearson's r`, also know as Pearson's correlation coefficent, or even as *the* correlation coefficient, is a number ion the range -1..+1 that indicates the relationship between two lists of numbers. If the value is +1, the numbers increase in value at the same rate in each list; if the number is -1, the numbers in one list grow at the same absolute rate as the numbers decrease in the other list. If the coefficient is zero, there is no relationship in the way the numbers go up and down in each list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-danger"
    ]
   },
   "source": [
    "*This is incorrectly referred to in the VLE materials as Pearson's $R^2$, although it may be referred to as Pearson's R.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pearsonr` function calculates Pearson's *R* value of correlation. The function takes two lists of numbers, of equal lengths. The Pearson's *R* function looks at the values at the same index in both lists and finds how the values in one column vary with respect to the other column. \n",
    "\n",
    "Note that we have to give each accident on its own row: if there are 145,000 accidents, the `pearsonr` function must be passed lists with 145,000 items.\n",
    "\n",
    "Recall that values near +1 show good positive correlation, values near -1 show good negative correlation, and values near 0 show no particular correlation. The `scipy` function also returns a second value, the *(2-tailed) p* value of the result, which gives a measure of the so-called *\"statistical significance\"* of the result. In this case, the *p* value is a measure of the likelihood that the effect the correlation coefficient measures does actually exist (i.e. whether that degree of correlation really is how correlated the lists are) or whether the degree of correlation measured is as likely to have occurred in two random lists of numbers.\n",
    "\n",
    "*(Generally, the closer the *p* value is to zero, the more unlikely it is that the effect indicated by the correlation coefficient does not actually exist; which is to say, the less likely it is that the measured effect does not hold, or the more likely it is that the reported effect does hold.)*\n",
    "\n",
    "The function is applied simply by passing in the two aligned lists of data whose correlation we wish to measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(r, p) = scipy.stats.pearsonr(cas_veh_df['Number_of_Casualties'],\n",
    "                              cas_veh_df['Number_of_Vehicles'])\n",
    "\n",
    "print('r: {r}, p: {p}'.format(r=r, p=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows a small, positive correlation (`r`) with a very small *p* (`p`) value.\n",
    "\n",
    "In other words, there's not much correlation, and the result is statistically significant (highly unlikely that the effect measured occurred by chance). This means we can reject the the null hypothesis that the number of casualties in an accident is unrelated to the number of vehicles.\n",
    "\n",
    "Looking at the data, it seems to be a result that most accidents result in very few casualties, and the accidents with the most casualties have few vehicles.\n",
    "\n",
    "Can you think of a reason for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Your thoughts here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-warning"
    ]
   },
   "source": [
    "*Learn more about this in the optional notebook `Using statistical tests â€” regression (optional)`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman's Rank Correlation Coefficient, $\\rho$\n",
    "\n",
    "Ages of people in the accidents dataset are stored as bands, not continuous values. This means that correlations between them must use a different correlation measure, Spearman's *rank* correlation coeffient, or Spearman's $\\rho$ (\"Spearman's rho\" (pronounced: row as in in boat).\n",
    "\n",
    "This measure again takes two lists of values, but rather than calculating the simple Pearson correlation coefficient, it first ranks the values in each list, and then measures the correlation between the ranked values.\n",
    "\n",
    "Similar to the Pearson function above, the `scipy.stats.spearmanr()` function takes two parameters, each a list of values for the two variables being compared, to calculate the rank correlation between the lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spearman's rank correlation is useful when we can order â€” that is, *rank* â€” items, but where the quantity we are ranking may not itself be distributed in a convenient way or may not provide numeric values with suitable intervals that we could use as part of Pearson's r calculation.\n",
    "\n",
    "One such example might be where we have age bands over different ranges, such as `[under18, 18-25, 26-40, 41-65, 65+]`. In this case we might use the rank of each group in the ordered list of groups for the purpose of correlation calculations, rather than try to associate an a single specific age with each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *accidents* dataset, an age range field field is referred to from the `'Age_Band_of_Driver'` and `'Age_Band_of_Casualty'` fields.\n",
    "\n",
    "The `Age_Band_of_Driver` labels are defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=accidents_db['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.find_one({'label': 'Age_Band_of_Driver'})['codes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the `Age_Band_of_Casualty` codes we have met before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.find_one({'label': 'Age_Band_of_Casualty'})['codes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-danger"
    ]
   },
   "source": [
    "*A `-1` null code also exists for situations where the age is missing or not known but this value is not defined in the `labels` collection.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age range bands have a natural rank order, but may have arbitrary code values. (In actual fact, the code values for these groups (the index values) *are* meaningfully numerically ordered, but let's imagine for now that they aren't.)\n",
    "\n",
    "How might we go about identifying whether or not there is any correlation between the ages of a driver and a passenger, at least according to the accidents database?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "*Add your thoughts here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way in to exploring this question might be to generate a table where each row has two values: one for the age band of the driver, and one for the age band of a passenger.\n",
    "\n",
    "Calculating Spearman's rank correlation coefficient across these two columns should identify whether a relationship holds between them; for example, as far as vehicles involved in accidents go, do young drivers have young passengers, are old drivers associated with old passengers, etc.\n",
    "\n",
    "The next few setions and activities are used to construct just such a dataframe, before we move on to calculating the rank correlation between the age range values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-squared ($\\chi^2$) example 1: hypothetical voting intention\n",
    "This is the same example as used in the VLE teaching material, based on gender related voting across different political parties, showing how the chi-squared ($\\chi^2$) statistic is calculated.\n",
    "\n",
    "Let's start by creating a DataFrame from a simple `dict`, where each entry is a column in the DataFrame. The key is the column name, the value is the items in the column. Each set of column values is itself a `dict`, with one key for each index entry and the value being the contents of that cell in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": false
   },
   "outputs": [],
   "source": [
    "actual_survey_results = pd.DataFrame({'Conservative': {'Men': 170, 'Women': 220},\n",
    "                                      'Labour': {'Men': 240, 'Women': 190},\n",
    "                                      'Other': {'Men': 80, 'Women': 100}})\n",
    "actual_survey_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ($\\chi^2$) test (sometimes also referred to as *Pearson's chi-squared test*) is a test applied to a so-called a contingency table to compare actual and expected counts between various observed categories.\n",
    "\n",
    "We could find the expected counts manually, or we could use the `scipy.stats.contingency.expected_freq()` function to do it for us. Note that this returns an array, rather than a DataFrame, but it's the same shape as the original DataFrame (that is, is has the same number of rows and columns, arranged in the same way).\n",
    "\n",
    "Here's how to use the `scipy.stats` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.contingency.expected_freq(actual_survey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then wrap this in a DataFrame labeled using the same column and index values of the source DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_array = scipy.stats.contingency.expected_freq(actual_survey_results)\n",
    "pd.DataFrame(_array, columns=actual_survey_results.columns).set_index(actual_survey_results.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could create our own function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_of_df(actual_df):\n",
    "    df = pd.DataFrame(\n",
    "        {c: \n",
    "         {r: actual_df[c].sum() * actual_df.loc[r].sum() / actual_df.sum().sum()\n",
    "                  for r in actual_df[c].index} \n",
    "              for c in actual_df})\n",
    "    # Fix the order of columns and rows\n",
    "    df = df[actual_df.columns]\n",
    "    df = df.reindex(actual_df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then check that we get a similar result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_survey_results = expected_of_df(actual_survey_results)\n",
    "expected_survey_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we're using a table of several rows and columns, we use the `scipy.stats.chi2_contingency()` function to find the $\\chi ^ 2$ statistic and the *p* value. \n",
    "\n",
    "Note that the function returns $\\chi ^ 2$, the *p* value, the number of degrees of freedom, and the matrix of expected frequencies. We're generally after just the second returned value, the *p* value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.chi2_contingency(actual_survey_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p, _, _ = scipy.stats.chi2_contingency(actual_survey_results)\n",
    "'chi2: {}, p: {}'.format(chi2, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *p* value of 0.0009, a very small value, suggests that the observed values are highly unlikely to have been the result of chance, and means that we can reject the null hypothesis that voting intention is independent of gender: for this example, it seems that we *can* say that men and women vote differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we adjust the numbers slightly, we can get a very different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": false
   },
   "outputs": [],
   "source": [
    "actual_survey_results_2 = pd.DataFrame({'Conservative': {'Men': 170, 'Women': 220},\n",
    "                      'Labour': {'Men': 220, 'Women': 210},\n",
    "                      'Other': {'Men': 80, 'Women': 100}})\n",
    "actual_survey_results_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_survey_results_2 = expected_of_df(actual_survey_results_2)\n",
    "expected_survey_results_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p, _, _ = scipy.stats.chi2_contingency(actual_survey_results_2)\n",
    "chi2, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *p* value of 0.07 means that there is a 7% chance that the observed values may have occurred by chance, which is more than the 5% (1 in 20) chance that we typically use to to decide whether something is \"statistically significant\";  that is, we *cannot* reject (at the 5% level) the null hypothesis that voting intention is independent of gender: for this modified example, we *can't* say that men and women vote differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi square example 2: accident frequency by day of week\n",
    "Let's look to see if more accidents occur on different days of the week. That is, from the observed number of accidents, how likely is it that the observed pattern of accidents occurred by chance, or does it seem to be the case that more accidents appear on some days than others?\n",
    "\n",
    "We'll start by counting the number of accidents that occurred on each day of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build a DataFrame, one row for each accident\n",
    "count_by_day_df = pd.DataFrame(accidents_collection.find({}, ['Day_of_Week']))\n",
    "\n",
    "# Find counts for each day\n",
    "count_by_day_ss = count_by_day_df['Day_of_Week'].value_counts()\n",
    "\n",
    "# Reorder by day of week, add labels.\n",
    "day_of_week_labels = labels.find_one({'label': 'Day_of_Week'})['codes']\n",
    "\n",
    "count_by_day_ss.sort_index(inplace=True)\n",
    "count_by_day_ss.index = [day_of_week_labels[str(r)] for r in count_by_day_ss.index]\n",
    "\n",
    "count_by_day_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualise the result to get a preliminary, gut reaction feel for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": false
   },
   "outputs": [],
   "source": [
    "count_by_day_ss.plot(kind='bar', color='royalblue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There look to be differences, but are they significant?\n",
    "\n",
    "We run into a slight problem here, though: the `scipy.stats.contingency.expected_freq` and `scipy.stats.chi2_contingency` functions we used in the voting example above assume the data is in a table of at least two rows and at least two columns.\n",
    "\n",
    "Whilst the calculations do run without error, the results are not informative.\n",
    "\n",
    "For example, the expected frequencies are the same as the actual ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.contingency.expected_freq(count_by_day_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the $\\chi^2$ value is 0, with a *p*-value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p, _, _ = scipy.stats.chi2_contingency(count_by_day_ss)\n",
    "'chi2: {}, p: {}'.format(chi2, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we have to use rather less convenient functions to calculate the $\\chi^2$ and _p_ values.\n",
    "\n",
    "First, we explicitly find the expected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": false
   },
   "outputs": [],
   "source": [
    "expected_count_by_day_ss = pd.Series(count_by_day_ss.sum() / len(count_by_day_ss), \n",
    "                                  index=count_by_day_ss.index)\n",
    "expected_count_by_day_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the `scipy.stats.chisquare()` function to find the test results. Note that the *p* value is nicely labelled for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": false
   },
   "outputs": [],
   "source": [
    "scipy.stats.chisquare(count_by_day_ss, expected_count_by_day_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *p* value of zero shows that this is a statistically significant result â€” the observed values are highly unlikely to have been the result of chance â€” and from that we might conclude that the varying number of accidents by day is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "### Activity 1\n",
    "\n",
    "As well as effects by day of week, we might expect that accidents are more likely in bad weather. We might also expect that the weather conditions are likely to affect different roads differently, with bad conditions on high-speed roads having more of an impact on accident likelihood than low-speed (typically urban) roads (perhaps because reaction times are more compromised by the higher speed conditions or perhaps because of anticipated larger traffic volumes).\n",
    "\n",
    "If the weather affects all roads equally, we would expect to see the proportions of accidents in different weather conditions to be the same for different road speed limits. \n",
    "\n",
    "Use a chi-squared test to determine if the proportion of accidents in different weather conditions is independent of road speed.\n",
    "\n",
    "Note that this activity will require several stages, following the pattern above: finding the values for the different ranges of `Weather_Conditions` (from inspection of the `label_of` dictionary), extracting the data from the database into a DataFrame and calculating the expected values for each combination of speed limit and weather and finally calculating the chi-squared *p* value.\n",
    "\n",
    "We have broken the task down into several subtasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "student": true
   },
   "source": [
    "What are the weather types?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": false,
    "student": true
   },
   "outputs": [],
   "source": [
    "# Enter your code in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "student": true
   },
   "source": [
    "Retrieve the weather and speed data for each accident.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Enter your code in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "student": true
   },
   "source": [
    "Count the number of accidents for combination of speed and weather, and label the rows and columns with meaningful labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Enter your code in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "student": true
   },
   "source": [
    "Calculate the chi-squared value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Enter your code in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "*Comment here on the $\\chi^2$ value. What does it appear to tell you?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "student": true
   },
   "source": [
    "Write your comments in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "student": true
   },
   "source": [
    "Plot the data to see whether any possibly effects or features jump out at you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "student": true
   },
   "outputs": [],
   "source": [
    "# Enter your code in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "heading_collapsed": true
   },
   "source": [
    "#### Our solution\n",
    "\n",
    "To reveal our solution, run this cell or click on the triangle symbol on the left-hand side of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "First, we need to identify the different recorded weather conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label = 'Weather_Conditions'\n",
    "\n",
    "weather_conditions_labels = labels.find_one({'label': label})['codes']\n",
    "weather_conditions_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "Next we need to retrieve the weather and speed data for each accident:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build a DataFrame, one row for each accident\n",
    "speed_by_weather_df = pd.DataFrame(accidents_collection.find({}, ['Speed_limit', 'Weather_Conditions']))\n",
    "speed_by_weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "And count the number of accidents for each speed and weather combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Count the number of each severity\n",
    "speed_by_weather_df = pd.crosstab(speed_by_weather_df['Speed_limit'],\n",
    "                                  speed_by_weather_df['Weather_Conditions'])\n",
    "\n",
    "speed_by_weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "Label the rows and columns appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "speed_by_weather_df.columns = [weather_conditions_labels[str(w)] for w in speed_by_weather_df.columns]\n",
    "speed_by_weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "Calculate the $\\chi^2$ value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(chi2, p, _, _) = scipy.stats.chi2_contingency(speed_by_weather_df)\n",
    "\n",
    "print(f'Chi-squated: {chi2}, p: {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "The very small *p* value shows that this is a significant result: it seems likely that chance does not explain the observed distribution and so weather conditions do seem to affect accident rates differently on different roads.\n",
    "\n",
    "Note that the chi-squared test doesn't tell us anything about *how* the weather conditions affect accident rates, only that they do. Visualising the data may help us to see some sort of effect that we could then check using further statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ax = speed_by_weather_df.plot(kind='bar')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true,
    "hidden": true
   },
   "source": [
    "Hmmm... it's hard to see from that what's going on, but if we view the data using a log scale on the y axis (add the argument `logy=True` to the `.plot()` function) or limit the y-axis range (for example, pass `ylim=(0,1500)`) we can see more and different detail. To me, the shape of distribution of accidents by weather type does look sort of similar on each road type... More detailed investigation needed, methinks..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": true
   },
   "source": [
    "#### End of Activity 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "This is one of two optional notebooks. If you have time available, consider reviewing the other optional notebook. Otherwise, return to the module materials now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
