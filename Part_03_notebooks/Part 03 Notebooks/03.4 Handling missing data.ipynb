{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling missing data in SQL and *pandas*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook covers the basics of recognising and handling the standard encoding of missing data in SQL and *pandas*.\n",
    "\n",
    "We cannot cover the cases where the user has created sentinel values to capture semantic variations in missing data.  In these cases the reasons data is missing usually puts this activity in the data cleansing and harmonisation activities, requiring decisions about appropriate alternative representations, rather than recognising and manipulating missing values.\n",
    "\n",
    "As always, we encourage you to extend these Notebooks as you uncover or refine your techniques for handling specific examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data values in *pandas* are typically represented as `NaN` (not a number) sentinel values, which can be assigned as the built-in python value `None`, or by using the *numpy* (numerical python) library value `np.nan`.\n",
    "\n",
    "SQL uses `NULL`, and languages such as R tend to use `NA` as the null marker. \n",
    "We can achieve a similar effect by importing the *numpy* library (which is usually abbreviated to `np`, and then using the value `np.nan`. We can assign the value `np.nan` to the variable `NA`, so that `NA` appears in place of `nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NA=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the PostgreSQL database engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the notebook `03.3 combining data from multiple datasets`, in this notebook we will use the installed version of PostgreSQL for some of the data manipulations.\n",
    "\n",
    "As with that notebook, we first need to connect to the PostgreSQL system, and then have a way to tell Python to pass the SQL code to PostgreSQL evaluation and to copy back into the Notebook any results tables we wish to capture.\n",
    "\n",
    "For TM351, the connection is slightly different depending upon whether you are connecting to the Open University-hosted server (accessed through [tm351.open.ac.uk](https://tm351.open.ac.uk)), or a local server (accessed using docker or vagrant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the database on [tm351.open.ac.uk](https://tm351.open.ac.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the Open University hosted server, you should execute the following cell, using your OUCU as the value of `DB_USER`, and the password you were given at the beginning of the module.\n",
    "\n",
    "If you need to, you can obtain your password again here: https://students.open.ac.uk/mct/tm351."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the test file you ran at the beginning of the module, you will need to provide your authentication credentials to connect with the database.\n",
    "\n",
    "Click on the following cell, and replace the values `oucu123` and `tm351pwd` with your OUCU and your TM351 password (*not* your Open University password). Note that if the cell is in RAW NBconvert style, you will need to change its type to \"code\" in order to execute it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "student": true
   },
   "source": [
    "# If you are using the remote environment, change this cell\n",
    "# type to \"code\", enter your login credentials, and run it\n",
    "\n",
    "DB_USER='oucu123'\n",
    "DB_PWD='tm351pwd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the database on a locally hosted environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running the Jupyter server on your own machine, via Docker or Vagrant, you should execute the following cell. Note that if the cell is in RAW NBconvert style, you will need to change its type to Code in order to execute it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If you are using the local environment, change this cell\n",
    "# type to \"code\", and execute it\n",
    "\n",
    "DB_USER='tm351'\n",
    "DB_PWD='tm351'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The settings from the previous cells allow us to create a *connection string*, which is used to set up the connection to the database. First we will import pandas, and load in the sql extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can construct the connection string so that we can connect to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use urllib to escape any special characters in the password\n",
    "\n",
    "import urllib\n",
    "\n",
    "DB_CONNECTION_STRING='{engine}://{user}:{pwd}@{addr}/{name}'.format(engine='postgresql',\n",
    "                                                                    user=DB_USER,\n",
    "                                                                    pwd=urllib.parse.quote_plus(DB_PWD),\n",
    "                                                                    addr='localhost:5432',\n",
    "                                                                    name=DB_USER)\n",
    "\n",
    "#Preview the connection string\n",
    "DB_CONNECTION_STRING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And connect to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql $DB_CONNECTION_STRING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting missing data  into a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you worked through the `03.3 combining data from multiple datasets` Notebook you will already have seen that operations such as the outer joins can generate rows in `DataFrame`s and SQL tables that contain the `NULL` or `NaN` or `None` markers.\n",
    "\n",
    "It is also sometimes useful to be able to insert missing data markers directly into the `DataFrame` or table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In pandas we can use an appropriate sentinel value directly in place of an actual value.\n",
    "ss_df = pd.DataFrame( { 'key':['a', 'b', 'c', NA, 'e', 'f'], \n",
    "                        'num':[1, None, 3, 4, np.nan, 5] })\n",
    "# Notice that we've used 3 different representations of the no value \n",
    "# marker - NA, None and np.nan in the above,\n",
    "# but when they're displayed in the pandas Dataframe they're \n",
    "# rendered as NaN, the pandas representation.\n",
    "\n",
    "ss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL allows the NULL marker to be entered in most places where data values can be entered, specifically in the `INSERT INTO` command and `UPDATE` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS dummy;\n",
    "CREATE TABLE dummy(key INT, name VARCHAR(20), value REAL);\n",
    "INSERT INTO dummy VALUES(NULL,'This',12.1);\n",
    "INSERT INTO dummy VALUES(2, NULL,345.00);\n",
    "INSERT INTO dummy VALUES(3,'The other', NULL);\n",
    "\n",
    "SELECT * FROM dummy;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the returned object is rendered as a `DataFrame`, so we're seeing the output representation of missing data in the result (for some reason, using the numpy `None`) - but the PostgreSQL database table will have SQL NULL markers in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note here is that the `NaN`, `NA`, `NULL` and `None` etc. can be used whatever the datatype expected - it's a marker for missing data which is effectively typeless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding missing data markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important issues is working out how to deal with *missing* data, and that starts with finding it.\n",
    "\n",
    "Why do you think using a condition like  `name == NULL`, is unlikely to work?\n",
    "\n",
    "Well, if `NULL` represents missing data - that is a data element that literally has `no value` - how could one `no value` be the same as another `no value`?\n",
    "\n",
    "Both *pandas* and SQL take the same approach - having special conditions tests to identify if missing data markers are present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pandas* missing values can be identified using the `isnull()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the vector-style application of isnull() to test every value in ss_df:\n",
    "ss_df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL has a similar conditional expression  `IS NULL` (and its converse `IS NOT NULL`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM dummy\n",
    "WHERE name IS NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM dummy\n",
    "WHERE name IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing something with missing data\n",
    "\n",
    "*pandas* has several methods for handling `DataFrame`s with missing data.\n",
    "\n",
    "SQL is generally a little poorer in this respect, forcing you to do much of the manipulation yourself.\n",
    "We'll start by looking at the *pandas* methods, then see how we might shape a similar effect over an SQL table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *pandas*: replacing missing data with a value\n",
    "We can replace a null marker using the `fillna()` method. \n",
    "\n",
    "By default this returns a new object (that is, the original object we apply the method to will not be changed).  To make the original object change we can add the `inplace='True'` qualifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ss_df['num'].fillna(0))\n",
    "# Note that ss hasn't been changed.\n",
    "print(ss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several useful parameters to `fillna()` allowing a range of different filling actions.\n",
    "\n",
    "For more information, see the pandas docs: [pandas.DataFrame.fillna](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.fillna.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *pandas*: deleting rows containing missing data,  various forms\n",
    "We can drop rows containing an `NaN` value *anywhere in the row* using the `dropna()` method.  Once again, this creates a new object unless we add the `inplace=True` qualifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop just those rows where there is a missing value in a particular column, we can use the `subset` parameter to specify which columns are of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df.dropna(subset=['key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful parameter allows us to just drop rows where *all* the values are missing: `how='all'`.\n",
    "\n",
    "To drop a *column* that is filled with NA values rather than a *row*, use `how='all', axis=1`.\n",
    "\n",
    "For more information, see the pandas docs: [pandas.DataFrame.dropna](http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.dropna.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the SQL used is a `SELECT` query, we're effectively creating a new table (the result of the `SELECT` query); to get the equivalent of the 'in place' behaviour we will need to explicitly update or delete the affected rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL:  replacing missing data with a value\n",
    "\n",
    "Basically we use a `WHERE` clause to identify those rows with `NULL` in the specified column, and process those appropriately - note that because the `WHERE` clause will result in only the rows that have `NULL` being copied to the result, it is then necessary to `UNION` those with the rows that didn't contain `NULL`.  (I did say you had a lot more work to do!)  \n",
    "\n",
    "If the change is to create a new object then the `COALESCE()` function is used - this takes a series of arguments and returns the first one that is not null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT COALESCE(key, 99999) AS key, \n",
    "       COALESCE(name, 'THIS ONE HAS NO NAME') as name, \n",
    "       COALESCE(value, 0.0) as value\n",
    "FROM dummy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- If coalesce is not available.\n",
    "SELECT key, name, value\n",
    "FROM dummy\n",
    "WHERE value IS NOT NULL\n",
    "UNION\n",
    "SELECT key, name, 0.0\n",
    "FROM dummy\n",
    "WHERE value IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this change inplace requires an `UPDATE` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "UPDATE dummy\n",
    "SET value = 0.0 \n",
    "WHERE value IS NULL;\n",
    "\n",
    "-- and we can see the change has affected the dummy table\n",
    "SELECT *  FROM dummy;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL: deleting rows containing missing data,  various forms\n",
    "In SQL `SELECT` you can simply choose the rows you want to keep with increasingly complex condition statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Removing a row with NULL anywhere in it requires each column in the row to be checked \n",
    "-- for the presence of NULL.\n",
    "SELECT * \n",
    "FROM dummy\n",
    "WHERE key IS NOT NULL AND name IS NOT NULL AND value IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Remove a row with NULL in a specific column in that row. \n",
    "SELECT * \n",
    "FROM dummy\n",
    "WHERE name IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Remove a row with NULL in ALL the columns.\n",
    "SELECT * \n",
    "FROM dummy\n",
    "WHERE NOT( key IS NULL AND name IS NULL AND value IS NULL);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're forced to move to `UPDATE` and `DELETE` if we want the inplace effect of actually changing the underlying table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Removing a row with NULL anywhere requires each column in the row to be checked.\n",
    "DELETE FROM dummy\n",
    "WHERE key IS NULL OR name IS NULL OR value IS NULL;\n",
    "\n",
    "\n",
    "SELECT * FROM dummy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Oops I needed that - lets get the original table back\n",
    "DROP TABLE IF EXISTS dummy;\n",
    "CREATE TABLE dummy(key INT, name VARCHAR(20), value REAL);\n",
    "INSERT INTO dummy VALUES(NULL,'This',12.1);\n",
    "INSERT INTO dummy VALUES(2, NULL,345.00);\n",
    "INSERT INTO dummy VALUES(3,'The other', NULL);\n",
    "\n",
    "SELECT * FROM dummy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Remove a row with NULL in a specific column in that row.\n",
    "DELETE\n",
    "FROM dummy\n",
    "WHERE name IS NULL;\n",
    "\n",
    "\n",
    "SELECT * FROM dummy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Still needed that but let's add an entirely null row too!\n",
    "DROP TABLE IF EXISTS dummy;\n",
    "CREATE TABLE dummy(key INT, name VARCHAR(20), value REAL);\n",
    "INSERT INTO dummy VALUES(NULL,'This',12.1);\n",
    "INSERT INTO dummy VALUES(2, NULL,345.00);\n",
    "INSERT INTO dummy VALUES(3,'The other', NULL);\n",
    "INSERT INTO dummy VALUES(NULL, NULL, NULL);\n",
    "\n",
    "SELECT * FROM dummy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Remove a row with NULL in ALL the columns.\n",
    "DELETE \n",
    "FROM dummy\n",
    "WHERE key IS NULL AND name IS NULL AND value IS NULL;\n",
    "\n",
    "SELECT * FROM dummy;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL: dropping a column where all rows had NULL in that column\n",
    "SQL would be a bit messy for this.\n",
    "\n",
    "Firstly, to remove an entire column requires the `ALTER TABLE` command with the `DROP < column name >` action.   However, this has no conditional part, so you would need to put this into an SQL conditional statement. \n",
    "\n",
    "Secondly, finding that a column had NULL in every row would require something like counting the number of rows that did not have `NULL` in that column and seeing if it was zero, but then also checking that you aren't looking at an empty table (one with no rows!).\n",
    "\n",
    "Thirdly, the `IF` condition needed to wrap the condition around the `ALTER TABLE` statement is only available in PostgreSQL functions.\n",
    "\n",
    "And finally, parameterising the required function, so that you could apply this to different columns in different tables, would be ... difficult - the alternative would be to write a specific function for each table and column you might want to apply it to!\n",
    "\n",
    "So it's probably best just to note a quick way to test for an entirely `NULL` column, and the manual application of the `ALTER TABLE` statement.\n",
    "\n",
    "No promises that this will work in all situations but this might do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Custom table, with one column entirely NULL\n",
    "DROP TABLE IF EXISTS dummy2;\n",
    "CREATE TABLE dummy2(key INT, name VARCHAR(20), value REAL);\n",
    "INSERT INTO dummy2 VALUES(NULL,'This',NULL);\n",
    "INSERT INTO dummy2 VALUES(2, NULL,NULL);\n",
    "INSERT INTO dummy2 VALUES(3,'The other', NULL);\n",
    "INSERT INTO dummy2 VALUES(NULL, NULL, NULL);\n",
    "\n",
    "SELECT * FROM dummy2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- The first part of the condition checks this is not an empty table.\n",
    "--                                         i.e. a table with no rows.\n",
    "-- The second part counts the number of rows where value is not null.\n",
    "--                               If this is 0 then they are all null.\n",
    "-- The 'dummy' outer SELECT simply returns the result of the condition.\n",
    "\n",
    "-- So, if the table is not empty and there are 0 rows where value is not null\n",
    "--        then you want to uncomment the code in the following cell\n",
    "\n",
    "SELECT ( ( (SELECT COUNT(*) FROM dummy2) <> 0)\n",
    " AND\n",
    " ( (SELECT COUNT(*) FROM dummy2 WHERE value IS NOT NULL) = 0 ) );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing with missing data elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally - what happens to some standard processing if there is missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- First let us remind ourselves what the dummy2 table looks like.\n",
    "SELECT *\n",
    "FROM dummy2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply some expressions and aggregations using rows and columns with `NULL` markers in them, and see what happens.\n",
    "\n",
    "Try to predict what you think will happen before looking at the result of running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT (key + 1) AS plusone, name\n",
    "FROM dummy2;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT name || ', add this to every name' AS nameandstring\n",
    "-- Note that the || is the string concatention operator in SQL.\n",
    "FROM dummy2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "-- This one usually catches people out.\n",
    "\n",
    "SELECT COUNT(key) as number_of_keys, COUNT(name) as number_of_names, COUNT(*) as number_of_rows\n",
    "FROM dummy2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT SUM(key) as total_of_keys\n",
    "FROM dummy2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM dummy2\n",
    "ORDER BY key;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as an exercise, why not find out what *pandas* does for the equivalent processing operations when `NAN`s are involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous notebook, we have created some new tables here. We will not need them again, so we'll remove them before going further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS dummy;\n",
    "DROP TABLE IF EXISTS dummy2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic techniques for identifying missing data (`NULL`, `None`, `NaN`, etc.) are similar in SQL and *pandas*: recognise that the value is missing by testing for it, then use that knowledge to decide what to do to resolve the missing data.\n",
    "\n",
    "There is also a standard set of *pandas* functions to operate on rows or columns containing NULLs; in SQL it's quite a bit harder to get the same effect - but that's because SQL and *pandas* were created for different reasons.  SQL is about the careful management and persistence of data, where restructuring and reshaping is rare; _pandas_ is about data analyse where reshaping and restructuring is vital.\n",
    "\n",
    "The module material talks about using different sentinel values (i.e. special strings like 'not known'). But if you do this you will need to write functions and conditions that identify these values as representing specific conditions in the dataset - and then use these to identify the data as requiring special treatment.  Some specialist libraries already have these kinds of sentinel values and the functions required to operate on them.\n",
    "\n",
    "In all cases, care will be needed to ensure you understand how missing data will be handled in complex expressions, calculations, aggregations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working through this Notebook as part of an inline exercise, return to the module materials now.\n",
    "If you are working through this set of Notebooks as a whole, move on to the Part 4 Notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
