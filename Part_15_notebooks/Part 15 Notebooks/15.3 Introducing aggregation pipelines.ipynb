{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Aggregation Pipelines\n",
    "\n",
    "This Notebook is about getting to know MongoDB's analysis pipeline as applied to the `accidents` dataset.\n",
    "\n",
    "In a previous notebook, you saw how the memory footprint of a MongoDB results cursor remained constant, irrespective of how many query result items might be associated with the cursor.\n",
    "\n",
    "We also mentioned how in many data processing systems it may be preferable to try to run computations over large datasets as close to the data as we can, rather than having to consume bandwidth and local memory when analysing the data.\n",
    "\n",
    "In this notebook, you will learn how we can make use of MongoDB aggregation pipelines to process data through cursors rather than downloaded large amounts of data and processing it via *pandas* dataframes.\n",
    "\n",
    "Pipelines allow you to define a sequence of steps that can be used to query a Mongo database and process the records that are returned.\n",
    "\n",
    "The pipeline may have multiple steps. The same operator may be used in multiple steps.\n",
    "\n",
    "As ever, load in some required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": false
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the document database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebooks for parts 14, 15 and 16, you will be using a document database to manage data. As with the relational database you looked at in previous sections, the data in the database is *persistent*. The document database, MongoDB, is described as \"NoSQL\" to reflect that it does not use the tabular format of the relational database to store data. However, many of properties of a formal RDBMS apply to MongoDB, including the need to connect to the database server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with PostgreSQL, the MongoDB database server runs independently from the Jupyter notebook server. To interact with it, you need to set up an explicit connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting your database credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with a database, we need to create a *connection* to the database. A connection allows us to manipulate the database, and query its contents (depending on what usage rights you have been granted). For the SQL notebooks in TM351, the details of your connection will depend upon whether you are using the OU-hosted server, accessed via [tm351.open.ac.uk](https:tm351.open.ac.uk), or whether you are using a version hosted on your own computer, which you should have set up using either Vagrant or Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the connection, you need a login name and a pasword. we will use the variables `DB_USER` and `DB_PWD` to hold the user name and password respectively that you will use to connect to the database. Run the appropriate cell to set your credentials in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to the database on [tm351.open.ac.uk](https:tm351.open.ac.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the Open University hosted server, you should execute the following cell, using your OUCU as the value of `DB_USER`, and the password you were given at the beginning of the module. Note that if the cell is in RAW NBconvert style, you will need to change its type to Code in order to execute it.\n",
    "\n",
    "The variables `DB_USER` and `DB_PWD` are strings, and so you need to put them in quotes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If you are using the remote environment, change this cell\n",
    "# type to \"code\", and execute it\n",
    "\n",
    "DB_USER='xxx99'            # Enter your OUCU here (in quotes)\n",
    "DB_PWD='your_password'     # Enter your password here (in quotes)\n",
    "\n",
    "import urllib\n",
    "\n",
    "MONGO_CONNECTION_STRING = f\"mongodb://{DB_USER}:{urllib.parse.quote_plus(DB_PWD)}@localhost:27017/?authsource=user-data\"\n",
    "print(f\"MONGO_CONNECTION_STRING = {MONGO_CONNECTION_STRING}\")\n",
    "\n",
    "DB_NAME=DB_USER\n",
    "print(f\"DB_NAME = {DB_NAME}\")\n",
    "\n",
    "ACCIDENTS_DB_NAME=\"accidents-2012\"\n",
    "# ACCIDENTS_DB_NAME=\"accidents-09-12\" # Uncomment this line to use the full accident database\n",
    "\n",
    "print(f\"ACCIDENTS_DB_NAME = {ACCIDENTS_DB_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, note that the connection string contains an additional option at the end: `?authsource=user-data`. For the MongoDB setup that we are using here, this option tells Mongo where to look for the authentication database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to the database on a locally hosted machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running the Jupyter server on your own machine, via Docker or Vagrant, you should execute the following cell. Note that if the cell is in RAW NBconvert style, you will need to change its type to Code in order to execute it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If you are using a locally hosted environment, change this cell\n",
    "# type to \"code\", and execute it\n",
    "\n",
    "MONGO_CONNECTION_STRING = f\"mongodb://localhost:27017/\"\n",
    "print(f\"MONGO_CONNECTION_STRING = {MONGO_CONNECTION_STRING}\")\n",
    "\n",
    "DB_NAME=\"test_db\"  # For a local VCE, this can be any value\n",
    "print(f\"DB_NAME = {DB_NAME}\")\n",
    "\n",
    "ACCIDENTS_DB_NAME=\"accidents\"\n",
    "print(f\"ACCIDENTS_DB_NAME = {ACCIDENTS_DB_NAME}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the locally hosted versions of the environment give you full administrator rights, which is why you do not need to specify a user name or password. Obviously, this would not generally not be granted on a multi-user database, unless you are the database administrator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set up a connection to the database. As with PostgreSQL, we use a connection string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MONGO_CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connection string is made up of several parts:\n",
    "\n",
    "- `mongodb` : tells `pymongo` that we will use MongoDB as our database engine\n",
    "- Your user name and (character escaped) password, separated by a colon if you are using the remote server. If you are using a local server, you will be logged on as an adminstrator, and do not need to specify a name or password.\n",
    "- `localhost:27017` : the port on which the database engine is listening.\n",
    "- A reference to the authentication file (`?authsource=user-data`), if you are using the remote server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now connect to the database with a `pymongo.MongoClient` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client=MongoClient(MONGO_CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be connected to the MongoDB database server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The accidents database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accidents database takes a long time to set up, so we have already imported it into a MongoDB database so that you can work with it. Note that on the remote VCE, the database is read-only, so you will not be able to alter its contents, although you can copy the contents into your own database space as discussed in the previous MongoDB notebooks, and alter that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells in the earlier section, Setting up the document database, put the name of the accidents database into the variable `ACCIDENTS_DB_NAME`. Use this value to set up the connection to the `accidents` database and collections within it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_db=mongo_client[ACCIDENTS_DB_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the names of the collections in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_db.list_collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will introduce some of the different collections in the rest of the materials, but let's start with the `accidents` collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_collection=accidents_db['accidents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This collection contains information on individual accidents. We can see how many examples it contains with the `.count_documents()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_collection.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also specify the `labels` collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=accidents_db['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be plotting some charts, so increase the default plot size to make things easier to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a larger plot size than the default\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting items using an aggregation pipeline\n",
    "\n",
    "First, an example of using an aggregation pipeline to get you started.\n",
    "\n",
    "Let's just check in advance how many accidents there are in the database for the Milton Keynes Highway Authority area (`E06000042`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_collection.count_documents({'Local_Authority_(Highway)': 'E06000042'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation pipelines are constructed from an sequence of pipleine operations. These operations may include, but are not limited to, selection operations, projections and grouping operations.\n",
    "\n",
    "We can create a pipeline to find a set of items using a single `$match` pipeline stage:\n",
    "\n",
    "```python\n",
    "# Define a pipeline stage\n",
    "select_stage = {'$match': {'Local_Authority_(Highway)': 'E06000042'}}\n",
    "\n",
    "# The pipeline is a list of stages\n",
    "pipeline = [select_stage]\n",
    "\n",
    "# Run a collection through a pipeline\n",
    "accidents.aggregate(pipeline)\n",
    "```\n",
    "\n",
    "The pipeline itself is defined as a list which is then evaluated via the `.aggregate()` operation applied to a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [{'$match': {'Local_Authority_(Highway)': 'E06000042'}}]\n",
    "\n",
    "# Show totals for each speed.\n",
    "mk_accidents = pd.DataFrame(accidents_collection.aggregate(pipeline))\n",
    "mk_accidents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the record count — it should match the previously reported value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mk_accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a simple query, we can create compounded queries in a `.find()` statement by just adding more fields to the initial selection dictionary. We can do the same in a pipeline.\n",
    "\n",
    "For example, let's check out some weather types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.find_one({'label': 'Weather_Conditions'})['codes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many Milton Keynes accidents were there in high winds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_collection.count_documents({'Local_Authority_(Highway)': 'E06000042',\n",
    "                           'Weather_Conditions': {\"$in\": [4, 5, 6]}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a similar approach with a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [{'$match': {'Local_Authority_(Highway)': 'E06000042',\n",
    "                        'Weather_Conditions': {\"$in\": [4, 5, 6]}}}]\n",
    "\n",
    "# Show totals for each speed.\n",
    "mk_high_winds = pd.DataFrame(accidents_collection.aggregate(pipeline))\n",
    "len(mk_high_winds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting the Amount of Data Flowing Through A Pipeline\n",
    "\n",
    "When testing a pipeline step, sometimes all we need is a single record, or a low number of records, to test it.\n",
    "\n",
    "To limit the data flowing into a pipeline, you can use a step of the form:\n",
    "\n",
    "`_limit1 = {'$sample': {'size': 1}}`\n",
    "\n",
    "and include it as the first step in the pipeline.\n",
    "\n",
    "If you need more records to test your pipeline step, increase the sample `size`.\n",
    "\n",
    "You can use a similar trick as the last step in a pipeline to limit the amount of data emitted from the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [{'$match': {'Local_Authority_(Highway)': 'E06000042',\n",
    "                        'Weather_Conditions': {\"$in\": [4, 5, 6]}}},\n",
    "            {'$sample': {'size': 3}}]\n",
    "\n",
    "pd.DataFrame(accidents_collection.aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projections in the Pipeline\n",
    "\n",
    "As well as limiting the \"length\" of data returned from the pipeline (that is, the number of items returned), we can also limit the \"width\" by projecting just the fields we are interested in.\n",
    "\n",
    "The `$project` operator allows us to define a projection determining fields are presented in records returned from the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [{'$match': {'Local_Authority_(Highway)': 'E06000042',\n",
    "                        'Weather_Conditions': {\"$in\": [4, 5, 6]}}},\n",
    "            \n",
    "            {'$project': {'Weather_Conditions':1, 'Speed_limit':1,\n",
    "                          'Accident_Severity':1, 'Number_of_Vehicles':1,\n",
    "                          '_id':0 }},\n",
    "            \n",
    "            {'$sample': {'size': 3}}]\n",
    "\n",
    "pd.DataFrame(accidents_collection.aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `$unwind` Aggregation Operator\n",
    "\n",
    "If we inspect a single document from the `accidents` collection, we notice that certain fields, such as `Casualties` and `Vehicles` are not simple `attribute:value` pairs but instead may contain a list of dictionaries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_collection.find_one({ 'Number_of_Casualties': {'$eq':2}}, {'_id':1, 'Casualties':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may recall that the *pandas* `.explode()` function will \"unravel\" a list within a particular column to create new rows, one per list item with the other column values retained.\n",
    "\n",
    "For example, given the following dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"items\": [\"item1\", \"item2\"],\n",
    "                    \"listed\": [[\"item1-element1\", \"item1-element2\"],\n",
    "                              [\"item1-element1\", \"item1-element2\"]]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explode the *listed* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explode(column='listed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `$unwind` aggregation operator ([docs](https://docs.mongodb.com/manual/reference/operator/aggregation/unwind/))  performs a similar operation to \"unwind\", \"unpack\" or \"expand\" an array list in a document to create a new set of documents, each of which contains *one* of the list elements along with the other items from the same original document.\n",
    "\n",
    "Let's see how it works using a simple example.\n",
    "\n",
    "To keep the example really simple, we'll limit the aggregation pipeline as follows:\n",
    "\n",
    "- retrieve just records containing two casualties using the `{'$match': <expr>}` operator: `{'$match': {'Number_of_Casualties': {'$eq':2}}}`;\n",
    "- sample just *two* of those records using the `{'$match': {'$size': INT}}` operator: `{'$sample': {'size': 2}}`;\n",
    "\n",
    "We'll then use the `$unwind` operator to unwind the casualties, and a `$project` operator to limit the scope of the fields we return in the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = [{'$match': {'Number_of_Casualties': {'$eq':2}}},\n",
    "            {'$sample': {'size': 2}},\n",
    "            {'$unwind': '$Casualties'},\n",
    "            {'$project': {'_id':0, 'Accident_Index':1, 'Casualties':1}}]\n",
    "\n",
    "unwound_casulaties = list(accidents_collection.aggregate(pipeline))\n",
    "unwound_casulaties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we normalise those results to a *pandas* dataframe, we get one row per casualty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(unwound_casulaties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "If you are working through this Notebook as part of an inline exercise, return to the module materials now.\n",
    "\n",
    "If you are working through this set of Notebooks as a whole, move on to `15.4 Grouping and summarising operations in aggregation pipelines`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
