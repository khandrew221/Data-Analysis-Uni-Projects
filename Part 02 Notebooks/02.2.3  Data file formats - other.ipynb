{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other data file formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, you will learn how to work with a variety of other file formats. Details for some file formats are left deliberately sparse. If you find yourself spending a lot of time working with such file formats, feel free to add additional notes to this Notebook, or create a new Notebook to record the recipes you find useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spreadsheet files (Excel XLS and XLSX files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although spreadsheet files are one of the most widely used file formats for sharing data, we have relegated them to this Notebook because we want you to get into the habit of using other file formats to publish and request data yourself.  \n",
    "\n",
    "Part 7 of the module looks at some of the weaknesses for analysis and management of data in spreadsheet form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one of the most widely used spreadsheet applications, the file formats used by Microsoft Excel by default are the ones most commonly encountered. Excel spreadsheet files can be recognised from the file extensions `.xls` and `.xlsx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open a file from a spreadsheet into a *pandas* DataFrame using the `read_excel()` function.\n",
    "\n",
    "To start with, load in the *pandas* package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to import a sheet directly into pandas using the `.read_excel()` method. Setting the sheetname parameter to `None` allows us to load in all the sheets as a `dict` of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following spreadsheet is taken from the Greater London Authority, London DataStore.\n",
    "#                     https://londondatastore-upload.s3.amazonaws.com/tfl-buses-type.xls\n",
    "#                     [retrieved 20/07/15]\n",
    "\n",
    "#Set the sheetname parameter to None to load in all the sheets as a dict of dataframes\n",
    "xl = pd.read_excel('data/tfl-buses-type.xls',  sheet_name=None)\n",
    "\n",
    "\n",
    "xl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify the sheets that have been loaded in as the `dict` keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the first few rows of the `Data` sheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl['Data'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can read in a single sheet by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('data/tfl-buses-type.xls', sheet_name='Data')\n",
    "\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting this data, or by opening the spreadsheet using a spreadsheet application or the OpenRefine tool (which is introduced in Part 2 of the module), we can check to see how many of the first few rows are metadata or blank rows. We can discount a certain number of lines at the top of the sheet using the `skiprows` parameter, or we can specify the spreadsheet row number of the header row explicitly and ignore the rows preceding that one. We can also define which columns we wish to import.  \n",
    "\n",
    "The `NaN`s sometimes indicate that cells are empty, or contain formula or other 'non' value data. In the cells under those containing 'Single deck' and 'Double deck' and alongside the description in the final row, the `NaN`s are there because the cells have been merged into a single spreadsheet spanning cell.\n",
    "\n",
    "(For more information, see the documentation for the [*pandas* read_excel method]( http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *xlrd*\n",
    "\n",
    "The `xlrd` package is a powerful package for reading and writing files using Excel's `.xls` and `.xlsx` formats, and lower level access to the contents of Excel spreadsheets than `pandas` provides. \n",
    "\n",
    "For more details see: http://xlrd.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "\n",
    "workbook = xlrd.open_workbook('data/tfl-buses-type.xls')\n",
    "# The library also allows us to preview the sheet names.\n",
    "print(workbook.sheet_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By manual inspection of the originally previewed sheet, we can use \n",
    "# xlrd to read the metadata from the metadata cell.\n",
    "# Note that row/columns indices are integer values, indexed on 0, \n",
    "# and also note that some cells span multiple rows.\n",
    "sheet = workbook.sheet_by_name('Data')\n",
    "sheet.cell_value(rowx=14, colx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing XML data into a *pandas* `DataFrame` is currently a little trickier than importing JSON, as there are no default *pandas* methods for supporting the import.\n",
    "\n",
    "Instead, you need to load in a file, parse it using a third party parser such as `lxml`, and then handle the mapping to the `DataFrame` yourself.\n",
    "\n",
    "Alternatively, use OpenRefine to parse the elements of the XML document that you are interested in and then save the data out again as a tabular CSV document which is a little easier to import."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to limit our use of XML-based datasets in this module, preferring instead CSV formats for tabular data and JSON for more elaborately structured datasets. You will, however, work with a particular style of XML later in the module when you look at Linked Data and the semantic web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing worth bearing in mind is that popular versions of XML formats may have Python libraries defined to make it easier to parse them, and read and write files defined using the format. For example, the KML format that is used to transport geographical data (points, lines, boundaries) can be parsed using the `fastkml` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Working with KML Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load in data from a KML file (a file format for geographic data sets) and then render it onto a map quite easily.\n",
    "\n",
    "For example, in the data directory is a file, `CarParks.kml` that contains a list of car park  locations on the Isle of Wight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fastkml` package provides various tools for parsing KML files and manipulating related data structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkml import kml\n",
    "k = kml.KML()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to open the file as a bytestream - and let the `lxml` parser used by the `fastxml` package identify the encoding itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = open(\"data/CarParks.kml\",'rb').read()\n",
    "k.from_string(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach is to open the file with a UTF-8 encoding to get a Unicode string, then throw away the first line that declares the decoding to be UTF-8. (The `.from_string()` function simply expects a KML document without the XML encoding prefix.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 3 data/CarParks.kml\n",
    "doc = open(\"data/CarParks.kml\", encoding='utf-8')\n",
    "lines = '\\n'.join(doc.readlines()[1:])\n",
    "k.from_string(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can parse the locations of the carpark placemarks from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placemarks = []\n",
    "\n",
    "for feature in k.features():\n",
    "    for placemark in feature.features():\n",
    "        placemarks.append((placemark.name, placemark.geometry.y, placemark.geometry.x))\n",
    "\n",
    "placemarks[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a simple `DataFrame` from these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_placemarks = pd.DataFrame(placemarks)\n",
    "df_placemarks.columns = ['Name', \"Latitude\", \"Longitude\"]\n",
    "\n",
    "df_placemarks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly map the markers to show how the parser has pulled out the placemark information. The `folium` package provides a set of tool for creating interactive maps, and adding markers to them, quite straightforwardly. (You will meet `folium` again in more detail in Part 5 of the module.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "alert-warning"
    ]
   },
   "source": [
    "NOTE: `folium` uses an external tileset to render the map background appearance. This requires that you have an internet connection when the map is being displayed, it may use cached tile data, but some tiles will be missing if you change scale by zooming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "If we know the latitude and longitude at the centre of the map we want to display, we can set it directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "carpark_map = folium.Map(location=[50.68, -1.2667], width = 960, height = 500, zoom_start=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the inbuilt operators of a *pandas* dataframe is the `mean()` operator. This can be used to calculate the mean value(s) for items in one or more numerically datatyped columns.\n",
    "\n",
    "We can use this operator to calculate the mean latitude and longitude of the points we wish to plot directly from the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mean, lon_mean = df_placemarks[['Latitude', 'Longitude']].mean()\n",
    "lat_mean, lon_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To place markers on a map, we can create a simple function that places a single marker given a latitude and longitude, and then apply that to each row of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbval-ignore-output"
    ]
   },
   "outputs": [],
   "source": [
    "def add_marker(row):\n",
    "    \"\"\"Add a marker to a map.\"\"\"\n",
    "    folium.CircleMarker(location=(row['Latitude'], row['Longitude']),\n",
    "                        popup=row['Name'],\n",
    "                        radius=20,\n",
    "                        fill_color='blue',\n",
    "                        fill_opacity=0.2\n",
    "                   ).add_to(carpark_map)\n",
    "\n",
    "\n",
    "#Apply the add_marker() function to each row (axis=1) of the dataframe\n",
    "df_placemarks.apply(add_marker, axis=1)\n",
    "\n",
    "carpark_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create the HTML file for the map, and display it below. (The HTML file can then be opened as a standalone file, outside of the Jupyter notebook context, directly from your browser.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carpark_map.save('data/IOWcarparlocations.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pandas* does not support YAML imports directly, but it is possible to use libraries such as the `PyYaml` library to load in a YAML file and convert it to a Python dict that can then be transformed to a *pandas* `DataFrame`.\n",
    "\n",
    "WARNING:  The `yaml.load()` and `yaml.load_all()` should not be used to parse arbitrary content from unsafe sources.  These functions are capable of creating arbitrary Python objects, including code.  The `yaml.safe_load()` and `yaml.safe_load_all()` limit that ability to objects that cannot generate executable code.\n",
    "\n",
    "As with XML, we will tend *not* to focus on the use of YAML, preferring instead JSON and CSV representations.\n",
    "\n",
    "The `yaml` package is one of many packages that can be used to open and parse YAML  files.\n",
    "\n",
    "`yaml.load()`  and `yaml.safe_load()` will both accept a single document string, and parse it to generate a python `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "document = \"\"\"\n",
    "image:\n",
    "    width: 800\n",
    "    height: 600\n",
    "    title:  View from 15th Floor\n",
    "    thumbnail:\n",
    "        url: http://www.example.com/image/481989943\n",
    "        height: 125\n",
    "        width:  100\n",
    "        animated : false\n",
    "    IDs:\n",
    "        - 116\n",
    "        - 943\n",
    "        - 234\n",
    "        - 38793\n",
    "\"\"\"\n",
    "parsedYAML = yaml.safe_load(document)\n",
    "parsedYAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `yaml.load()` and `yaml.safe_load()` functions will also accept a file name, open and read that file, and parse the contents into a Python `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = open('data/document.yaml', 'r') \n",
    "yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also cast a `dict` to YAML using the `yaml.dump()` function applied to a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml.dump(parsedYAML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in exploring Python's handling of YAML further, the `PyYAML` library documentation can be found at  http://pyyaml.org/wiki/PyYAMLDocumentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this Notebook you have seen how to:\n",
    "1. read .xls and .xlsx spreadsheet files\n",
    "2. handle XML files\n",
    "3. read KML files and seen map data plotted in folium\n",
    "4. parse YAML data and load it into a Python dict.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That completes the coverage of data file formats for this module; we will make extensive use of CSV and JSON formats in the module and may introduce others as we work through different tools and techniques.\n",
    "\n",
    "Return to the module materials now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
